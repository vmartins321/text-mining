{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel: Get data for text mining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-04T00:31:52.016381Z",
     "start_time": "2018-08-03T17:31:48.446789-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import datalab.storage as gcs\n",
    "from nltk import word_tokenize   \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T17:38:26.016868Z",
     "start_time": "2018-05-10T10:38:26.013899-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_id = 'my_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T21:49:16.987467Z",
     "start_time": "2018-05-10T14:49:16.984288-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query ='''\n",
    "SELECT \n",
    "pid\n",
    ", data.product.title\n",
    ", data.product.img_url\n",
    "FROM `data_science.product_catalog_distinct`\n",
    "WHERE data.categories.cat_4=\"Dresses\"\n",
    "AND data.product.title !=''\n",
    "AND data.categories.cat_1=\"Clothing, Shoes & Jewelry\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T21:50:02.403666Z",
     "start_time": "2018-05-10T14:49:18.083749-07:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Job ID: 0d4ddff8-02c8-4d31-ad8c-ea0c40104cba\n",
      "Query running...\n",
      "  Elapsed 7.72 s. Waiting...\n",
      "  Elapsed 8.89 s. Waiting...\n",
      "  Elapsed 10.07 s. Waiting...\n",
      "  Elapsed 11.28 s. Waiting...\n",
      "  Elapsed 12.47 s. Waiting...\n",
      "  Elapsed 13.65 s. Waiting...\n",
      "  Elapsed 14.82 s. Waiting...\n",
      "  Elapsed 16.01 s. Waiting...\n",
      "Query done.\n",
      "Processed: 10.7 GB Billed: 10.7 GB\n",
      "Standard price: $0.05 USD\n",
      "\n",
      "Retrieving results...\n",
      "Got 384995 rows.\n",
      "\n",
      "Total time taken 41.68 s.\n",
      "Finished at 2018-05-10 14:50:02.\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_gbq(query, dialect=\"standard\", project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mine keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T21:50:20.180903Z",
     "start_time": "2018-05-10T14:50:20.174516-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class Lemmatizer(object):\n",
    "     def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "     def __call__(self, doc):\n",
    "         return [self.wnl.lemmatize(t.replace(',', '')) for t in doc.split(' ') if len(t)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T21:50:21.123501Z",
     "start_time": "2018-05-10T14:50:21.118364-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents='ascii'\n",
    "                            , analyzer='word'\n",
    "                            , stop_words='english'\n",
    "                            , min_df=1000\n",
    "                            , max_df=0.50\n",
    "                            , max_features=100\n",
    "                            , tokenizer=Lemmatizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T21:50:59.825742Z",
     "start_time": "2018-05-10T14:50:23.976316-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = vectorizer.fit(data.title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T21:50:59.833653Z",
     "start_time": "2018-05-10T14:50:59.829549-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dress_keywords = [k for k,v in x.vocabulary_.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T21:50:59.856728Z",
     "start_time": "2018-05-10T14:50:59.850732-07:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1950s', '3/4', 'a-line', 'applique', 'backless', 'ball', 'bandage', 'beach', 'beaded', 'belt', 'big', 'black', 'blue', 'bodycon', 'boho', 'bridal', 'bride', 'bridesmaid', 'cap', 'casual', 'chiffon', 'club', 'cocktail', 'cotton', 'deep', 'elegant', 'evening', 'fit', 'flare', 'floral', 'flower', 'formal', 'girl', \"girls'\", 'gown', 'halter', 'high', 'homecoming', 'knee', 'lace', 'length', 'line', 'little', 'long', 'loose', 'maternity', 'maxi', 'mermaid', 'midi', 'mini', 'mother', 'navy', 'neck', 'party', 'pencil', 'pleated', 'plus', 'pocket', 'print', 'printed', 'prom', 'red', 'retro', 'round', 'ruched', 'ruffle', 'scoop', 'sequin', 'sexy', 'sheath', 'shirt', 'short', 'shoulder', 'size', 'skater', 'sleeve', 'sleeveless', 'slim', 'solid', 'spaghetti', 'split', 'strap', 'striped', 'style', 'summer', 'sweetheart', 'swing', 't-shirt', 'tank', 'tulle', 'tunic', 'u', 'v-neck', 'vintage', 'waist', 'wedding', 'white', 'woman', 'work', 'wrap']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(dress_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get keywords for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T17:40:38.523068Z",
     "start_time": "2018-05-10T10:39:59.072444-07:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victoria/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# Return a feature matrix of products, keywords\n",
    "tfidf_matrix = vectorizer.fit_transform(data.title) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T18:59:26.511711Z",
     "start_time": "2018-05-14T11:58:26.909774-07:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375141"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if the tfidf matrix is not empty for at least 100k products in the dataframe\n",
    "has_kw_count = 0\n",
    "for i in range(len(data)):\n",
    "    a = tfidf_matrix[i,:].nonzero()[1]\n",
    "    if a.any():\n",
    "        has_kw_count += 1\n",
    "has_kw_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:00:21.252452Z",
     "start_time": "2018-05-14T12:00:21.247949-07:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9854"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of products without keywords\n",
    "len(data) - has_kw_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T22:41:52.176829Z",
     "start_time": "2018-05-14T15:39:43.347621-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of products, keywords\n",
    "d = defaultdict(list)\n",
    "\n",
    "for doc in range(len(data)):\n",
    "    feature_index = tfidf_matrix[doc,:].nonzero()[1] # An array of keyword indices from the matrix\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index]) # A zip of indices array, scores\n",
    "    keyword_dict = defaultdict(list) # An empty dictionary\n",
    "    for k,v in tfidf_scores: \n",
    "        keyword_dict[k] = v # Turn the zip into a dictionary of index, score, probably can cut out the zip step\n",
    "    for k,v in keyword_dict.items():\n",
    "        d[doc].append(feature_dict.get(k)) # A dictionary of feature name, score; Takes just the name and appends to a doc dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add keywords back to products dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T22:41:56.792765Z",
     "start_time": "2018-05-14T15:41:55.132620-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the keywords dictionary into a dataframe with the product ids as the index\n",
    "k = pd.DataFrame.from_dict(d, orient = 'index') \n",
    "k = k.replace('None', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T18:30:35.184003Z",
     "start_time": "2018-05-15T11:30:35.179339-07:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9854\n"
     ]
    }
   ],
   "source": [
    "print(len(data)-len(k)) # The keyword dictionary method skips products without keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T22:43:56.324218Z",
     "start_time": "2018-05-14T15:43:56.205672-07:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat([data, k], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "121px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1091px",
    "left": "0px",
    "right": "951.648px",
    "top": "72px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
